## 1. Design DB schema and generate SQL
07/07/2023 (1h)

**[DBDiagram]( https://dbdiagram.io/d) -> Draw Entity-Relationship Diagrams and export it to PNG, SQL, etc...**

![[Pasted image 20230711085045.png]]

```sql
CREATE TABLE "accounts" (
"id" bigint PRIMARY KEY,
"owner_name" varchar NOT NULL,
"balance" decimal NOT NULL,
"currency" varchar NOT NULL,
"created_at" timestampz NOT NULL DEFAULT (now())
);

CREATE TABLE "entries" (
"id" bigint PRIMARY KEY,
"account_id" bigint,
"amount" decimal NOT NULL,
"created_at" timestampz DEFAULT (now())
);

CREATE TABLE "transfers" (
"id" bigint PRIMARY KEY,
"from_account" bigint,
"to_account" bigint,
"amount" decimal NOT NULL,
"created_at" timestampz DEFAULT (now())
);

COMMENT ON COLUMN "entries"."amount" IS 'Can be negative or positive';
COMMENT ON COLUMN "transfers"."amount" IS 'Must be positive';

ALTER TABLE "entries" ADD FOREIGN KEY ("account_id") REFERENCES "accounts" ("id");
ALTER TABLE "transfers" ADD FOREIGN KEY ("from_account") REFERENCES "accounts"("id");
ALTER TABLE "transfers" ADD FOREIGN KEY ("to_account") REFERENCES "accounts" ("id");
```

Indexes -> 
## 2. Install Docker & Postgres
11/07/2023 (1h)

Linux Alpine: A minimal Docker image based on Alpine Linux with a complete package index and only 5MB in size (most of images uses this as base)

### Docker Commands
```
// List docker containers
docker ps // See running containers
docker ps -a // See all container

// Pull docker image using the tag to specify the version
docker pull <image_name>:<tag>
docker pull postgres:15-alpine

// List docker images
docker images 

// Create a container from a docker image, also specifies the environment
// variable used within the container
docker run --name <container_name> -e <environment_variable> -d <docker_image> 
docker run --name postgres15 -p 5432:5432 -e POSTGRES_USER=root -e POSTGRES_PASSWORD=secret -d postgres:15-alpine

// Execute a command within a container
docker exec -it <container_name_or_id> <command> [args] 
docker exec -it postgres15 psql -U root

// Displays the logs of a docker container
docker logs <container_name_or_id> 
docker logs postgres15 

// Stop the container
docker stop <container_name_or_id> 

// Restart a container
docker start <container_name_or_id> 

// Access the container shell (sh, can be bash depending on the OS)
docker exec -it postgres15 /bin/sh

Postgres15 container ID: 42ac02d1df60e219dcc4b0528c01803e7f8c4dc1dc38d8ff3616fe5b3bcefa4e

// Create a PQSL database in the postgres container
docker exec -it postgres15 createdb --username=root --owner=root simple_bank

// Access the pqsl database in the container
docker exec -it postgres15 psql -U root simple_bank
```

## 3. Database Migration
12/07/2023 (1h)

A database migration is the modification of the database schema. When changing an attribute for instance, you will need to define a process in order to apply these changes but you want to avoid having to delete all your DB and recreate it to do it, that's when the database migration shines. It's a defined set of instructions for a specific action you want to apply on your database.

**golang-migrate -> Database migrations written in Go. Use as [CLI](https://github.com/golang-migrate/migrate#cli-usage) or import as [library](https://github.com/golang-migrate/migrate#use-in-your-go-project).**

```
migrate -version
migrate -help

migrate create -ext sql -dir db/migration -seq init_schema
```

This will generate two files:
- /db/migration/000001_init_schema.up.sql // Migration **UP**: Make a forward change to the schema
- /db/migration/000001_init_schema.down.sql Migration **DOWN**: Revert the change made by the UP Migration

### Makefile
```yaml
postgres:
docker run --name postgres15 -p 5432:5432 -e POSTGRES_USER=root -e POSTGRES_PASSWORD=secret -d postgres:15-alpine

createdb:
docker exec -it postgres15 createdb --username=root --owner=root simple_bank

dropdb:
docker exec -it postgres15 dropdb simple_bank

migrateup:
migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose up

migratedown:
migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose down

destroy:
docker stop postgres15
docker rm postgres15

.PHONY: postgres createdb dropdb migrateup migratedown destroy
```

```
// To create a new Simple Bank database in a Docker container
make postgres
make createdb
make migrateup
```

## 4. Generate CRUD Golang code from SQL | Compare db/sql, gorm, sqlx & sqlc
13/07/2023 (2h)

Multiple ways to interact with a database using go:
- Database/SQL native library (More control but can be tedious)
- GORM (CRUD functions already implemented, very short, must learn the way GORM, run very slowly on high load)
- sqlx (Quite fast, easy to use, field mapping via query text, failure won't occur until runtime)
- sqlc (very fast, easy to use, automatic code generation, catch SQL query before generating codes, full support for only PostGres)

```go
go mod init github.com/ngyale-pro/simplebank
go mod tidy
```

Example of a template for sqlc: 
```sql
-- name: CreateAccount :one

INSERT INTO accounts (
owner,
balance,
currency
) VALUES (
$1, $2, $3
) RETURNING *;

-- name: GetAccount :one
SELECT * FROM accounts
WHERE id = $1 LIMIT 1;

-- name: ListAccounts :many
SELECT * FROM accounts
ORDER BY id
LIMIT $1
OFFSET $2;

-- name: UpdateAccount :one
UPDATE accounts
set balance = $2
WHERE id = $1
RETURNING *;

-- name: DeleteAccount :exec
DELETE FROM accounts
WHERE id = $1;
```
- one: specify to return one result, can use RETURNING * to generate the code which will return the inserted row for instance
- many: specify to return multiple result
- exec: specify to execute a query, RETURNING * can't be used in this case (faster)

### 5.  Write Golang Unit Test for CRUD data with random data using testify
24/07/2023 (2h)

- Library used for testing: testing (internal) & testify (external)
	- "testing": Allow for testing without keywords such as assert, require, etc... -> Makes use of Fail, Fatal, Error, Log, etc... 
		- Has features such as fuzzy, parallel, perfomance testing
		- Recognize file as test files when ending with `_test.go`
		- Recognized function as test functions when starting with `Test[Method Name]`
	- "testify": Add several methods such as assert, require to help test to be more compact and concise. However it may be harder to debug and it adds an external dependency for testing (which all program need)
		- Difference between assert/require: if an assert fails, the test still continue/If a require fails, the test stops
```go
args := CreateEntryParams{
	AccountID: account.ID,
	Amount: util.RandomInteger(1, account.Balance),
}

// With internal testing library
entry, err := testQueries.CreateEntry(context.Background(), args)
if err != nil {
	t.Errorf("Error: %v", err)
}

if (entry == Entry{}) {
	t.Errorf("Error: The entry should not be empty")
}

if entry.AccountID != account.ID {
	t.Errorf("Error: The entry ID should be equivalent to the created account ID")
}

if entry.Amount != args.Amount {
	t.Errorf("Error: The entry Amount is not the same as the one given")
}

if entry.ID == 0 {
	t.Errorf("Error: The entry ID is zero")
}

if entry.CreatedAt.IsZero() {
	t.Errorf("Error: The entry creation date is zero")
}

// With external testify 
entry, err := testQueries.CreateEntry(context.Background(), args)

assert.NoError(t, err)
assert.NotEmpty(t, entry)
assert.Equal(t, entry.AccountID, account.ID)
assert.Equal(t, entry.Amount, args.Amount)
assert.NotZero(t, entry.ID)
assert.NotZero(t, entry.CreatedAt)
```

Assert vs Require: https://www.yellowduck.be/posts/assert-vs-require-in-testify
Comprehensive Guide to Testing in Go: https://blog.jetbrains.com/go/2022/11/22/comprehensive-guide-to-testing-in-go/#WritingCoverageTests
Go Example for Testing: https://gobyexample.com/testing
Testing in purpose errors: https://bitfieldconsulting.com/golang/testing-errors

- String Concatenation
	- To work with strings you can either use string or string builder
```go
import strings

// Without String Builder
func RandomString(n int) string {
	var generated_string = ""
	for i := 0; i < n; i++ {
	generated_string += string(alphabet[rand.Intn(26)])
	}
	return generated_string
}

// With String Builder
func RandomString(n int) string {
	var sb strings.Builder
	var alphabet_length = len(alphabet)
	for i := 0; i < n; i++ {
		sb.WriteByte(alphabet[rand.Intn(alphabet_length)])
	}
	return sb.String()
}
```
- Using String Builder, it way faster than just using the string type (e.g: For 500000 characters, it takes 140ms for SB and 8s for string)

String Builder: https://zetcode.com/golang/builder/
Go Example: https://gobyexample.com/string-functions
Check an empty struct: https://freshman.tech/snippets/go/check-empty-struct/

NB:
- Functions starts with an uppercase when it's a public function in your package (e.g: fmt.Println)
- It starts with a lowercase when it's only internal in your package (e.g: helper or utility)
	- For anything to be exported, it should start with an uppercase

### 6. A clean way to implement database transaction in Golang
26/07/2023

![[Pasted image 20230728145528.png|400]]
- DB Transaction
	- Single Unit of Work -> Reliable and consistent, even in case of system failure
	- Provide isolation between programs that access the database concurrently
	- ACID Property (Atomicity, Consistency, Isolation, Durability)
		- No partial operation -> DB State must be coherent at all time -> Transactions can run concurrently -> Every transaction will be stored in persistent storage 
	- Often made up of multiple db operations such as the following scenario (Account 1 transfer 10 to Account 2)
![[Pasted image 20230725004836.png|375]]

- Algorithm for transaction:
	- Initialize result variable
	- Start transaction: `db.beginTx(ctx, nil)`
	- Execute queries
		- In case of error: `tx.Rollback()`
	- Commit transaction: `tx.Commit()
	- Return result variable
```sql
BEGIN;
SELECT * from accounts WHERE id = 1 FOR UPDATE; // "Block concurrent read until the transaction is done"
UPDATE accounts SET balance = 10 WHERE id = 1
COMMIT;
```

```go
// Function as parameter and call it in the function
func (store *Store) execTx(ctx context.Context, fn func(*Queries) error) error {
	tx, err := store.db.BeginTx(ctx, nil)
	if err != nil {
		return err
	}

	q := New(tx)
	err = fn(q)
}

// Give function as param
store.execTx(ctx, func(q *Queries) error {
		var err error
		result.Transfer, err = q.CreateTransfer(ctx, CreateTransferParams{
			FromAccountID: arg.FromAccountID,
			ToAccountID:   arg.ToAccountID,
			Amount:        arg.Amount,
		})
		if err != nil {
			return err
		}
	}
}
```

- Channel
	- Enable communication between goroutines
	- Reading channel blocks the reader until receiving value
```go
// Declaration
channel := make (chan <T>)

// Use

func main() {
	msg_channel := make(chan string)
	nb_goroutines := 3
	for i := 0; i < nb_goroutines; i++ {
		go func () {
			// Sends a message in the channel
			msg_channel <- "Sending a message from the goroutine"
		}
	}
	// Reads a message from the channel, blocking until receiving messages
	msg := <- msg_channel
}
```
### 7. DB Transaction Lock & How to handle deadlock in Golang
27/07/2023 (3h)

- Lock Select Transaction -> Prevent two transactions to read the same resource which will be modified (by adding FOR UPDATE)

- Deadlock Detected: How to debug
	- Add print log to debug for each query
	- Pass struct as key of context to goroutines
```go
		transactionName := fmt.Sprintf("Transaction: %d", i)
		go func() {
			ctx := context.WithValue(context.Background(), transactionKey, transactionName)
			result, err := store.TransferTx(ctx, args)
```

- Reproduce the bug with only SQL
	- T2: Create Transfer
	- T2: Create Entry 1
	- T1: Create Transfer
	- T2: Create Entry 2
	- T2: Get Account 1 // Blocked, why ? 
	- T1: Create Entry 1
	- T1: Create Entry 2
	- T1: Get Account 1
	- T1: Update Account 1
```sql
"Scenario: Create on purpose a deadlock"
BEGIN 

INSERT INTO transfers (from_account_id, to_account_id, amount) VALUES (1, 2, 10) RETURNING *;

INSERT INTO entries (account_id, amount) VALUES (1, -10) RETURNING *;
INSERT INTO entries (account_id, amount) VALUES (2, 10) RETURNING *;

SELECT * FROM accounts WHERE id = 1 FOR UPDATE;
UPDATE accounts SET balance = 90 WHERE id = 1 RETURNING *;

SELECT * FROM accounts WHERE id = 2 FOR UPDATE;
UPDATE accounts SET balance = 90 WHERE id = 1 RETURNING *;
```

- Investigate postgres locks
```sql
"Display the table of pg locks to investigate"

SELECT
	a.application_name,
	l.relation::regclass,
	l.transactionid,
	l.mode,
	l.locktype,
	l.GRANTED,
	a.usename,
	a.query,
	a.pid
FROM pg_stat_activity a
JOIN pg_locks l ON l.pid = a.pid
WHERE a.application_name = 'psql'
ORDER BY a.pid;
```

SQL Lock Keyword:
- FOR UPDATE: Locks concurrent reads on the resource
- FOR NO KEY UPDATE: Locks concurrent reads on the resource, but as it guarantees foreign key won't be updated, other transaction which need to check the foreign constraint will grant the lock
- NO WAIT: The transaction rollback and return an error instead of waiting 
```sql
"Concurrent Read allowed between transaction -> Race concurrency leads to incoherent state"
SELECT * FROM accounts
WHERE id = $1 LIMIT 1;

"FOR Update blocks every concurrent reads on the resource by other transaction, but deadlock as foreign constraint can't retrieve the resource blocked"
SELECT * FROM accounts
WHERE id = $1 LIMIT 1
FOR UPDATE;

"FOR NO KEY UPDATE explicits the foreign key won't be changed, so no need to lock the resource"
SELECT * FROM accounts
WHERE id = $1 LIMIT 1
FOR NO KEY UPDATE;
```
For more details: 
- FOR UPDATE vs FOR NO KEY UPDATE: https://www.postgresdba.com/bbs/board.php?bo_table=C05&wr_id=186
- Acces Concurrent avec FOR UPDATE: https://www.bortzmeyer.org/select-for-update.html

Transform two queries into one (add to previous value):
```sql
-- name: AddAccountBalance :one

"Equivalent of retrieving the balance and adding a new amount"
SELECT * FROM accounts
WHERE id = $1 LIMIT 1;

UPDATE accounts
  set balance = $1
WHERE id = $2
RETURNING *;

"Equivalent of retrieving the balance and adding a new amount"
UPDATE accounts
  set balance = balance + sqlc.arg(amount)
WHERE id = sqlc.arg(id)
RETURNING *;

```

#### 8. How to avoid deadlock in DB transaction? Queries order matters!
27/07/2023 (1h)

Deadlock Scenario
```sql
BEGIN;
UPDATE accounts SET balance = balance - 10 WHERE id = 1 RETURNING *;
UPDATE accounts SET balance = balance + 10 WHERE id = 2 RETURNING *;

BEGIN;
UPDATE accounts SET balance = balance - 10 WHERE id = 2 RETURNING *;
UPDATE accounts SET balance = balance + 10 WHERE id = 1 RETURNING *;
```
- Solution: Change the order of queries to prevent deadlock
	- Reversing the second transaction queries will prevent deadlock as the first transaction won't be preventing to acquire the lock when executing the second query

####  9. Understand isolation levels & read phenomena in MySQL & PostgreSQL via examples
28/03/2023 (1h)

Read Phenomena:
- **Dirty Read**: A transaction reads data written by other concurrent uncommitted transaction
![[Pasted image 20230728150751.png|200]]
- **Non-repeatable Read**: A transaction reads the same row twice and sees different value because it has been modified by other committed transaction
![[Pasted image 20230728150935.png|200]]
- **Phantom Read**: A transaction re-executes a query to find rows that satisfy a condition and sees a different set of rows, due to changes by other committed transaction
	- In the following example, the query selection every account which balance is >= 90 will give 3 results. After the first transaction updates the balance of an account to 80 and commits, the same query selection will return 2 results instead of 3. That is a phantom read.
![[Pasted image 20230728151019.png|200]]
- **Serialization Anomaly**: The result of a group of concurrent committed transactions is impossible to achieve if we try to run them sequentially in any order without overlapping
	- In the following example, two transactions insert an account entry which its balance is the sum of all account. Due to the isolation level, they will be both able to create the entry but they will be duplicates. If we run them sequentially, there is no way this will lead to the same result, as the sum of the second transaction will be 540 and not 270. That is a serialization anomaly.
![[Pasted image 20230728154030.png|200]]

4 Standard Isolation Levels defined by ANSI (American National Standards Institute)
- **Read Uncommitted**: Can see data written by uncommitted transaction 
- **Read Committed**: Only see data written by committed transaction
- **Repeatable Read**: Same read query always return same result
> [!NOTE]- Difference of implementation between mysql and psql for repeatable read
	>  
	> For mysql in this isolation level, if two concurrent transactions wants to update the same 
	> value by adding ten, the first transaction committed the change, the second transaction 
	> will see the previous value when selecting it (repeatable read), but after the second 
	> transaction update the value with set value = value + 10, the resulted value will be value + 
	> 20. 
	> Other implementation such as psql will choose to refuse the transaction and throw an 
	> error instead as this may lead to incoherent values.
	> ![[Pasted image 20230728151800.png|200]]
- **Serializable**: Can achieve same result if execute transaction serially in some order instead of concurrency -> There is one way of ordering transactions that will always lead to the same result
	- In mysql, this translates by making any SELECT implicitely FOR SHARE which prevent any update on the value by other transactions, allowing only read. Doing this, forces the one writing the transaction to think ahead of the order of its queries in the transaction to prevent a deadlock.  
	- In pgsql this will throw an error ("Could not serialize access due to read/write dependencies among transactions.")
		- DETAIL: Reason Code: Canceled on identification as a pivot, during commit attempt.
		- HINT: The transaction might succeed if retried

![[Pasted image 20230728154748.png|200]]

```sql
"---For mysql---"
"Retrieve the level of isolation"
SELECT @@transaction_isolation;

"By default, it's repeatable read"

"For the session only, change the isolation level to read uncommited"
SET session transaction isolation level read uncommitted;

"---For postgresql---"
"Retrieve the level of isolation"
SHOW transaction isolation level;

"By default, it's read committed"

"In psql, the isolation level is set as the transaction level only"

BEGIN;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;

"In pgsql, read uncommitted behaves like read committed because of the pgsql architecture, meaning that only 3 levels of isolation exists in pgsql

```

- Select FOR SHARE vs SELECT FOR UPDATE
- The `select for share` prevents updates and deletes of rows, but doesn’t prevent other processes from acquiring a `select for share`. 
- On the other hand, `select for update` also blocks updates and deletes, but it also prevents other processes from acquiring a `select for update` lock.

For more details: 
- Share and Update in psql: https://shiroyasha.io/selecting-for-share-and-update-in-postgresql.html


![[Pasted image 20230728155349.png|225]]

![[Pasted image 20230728155308.png|525]]

|                             | mysql             | postgresql             |
| --------------------------- | ----------------- | ---------------------- |
| Number of Isolation Level   | 4                 | 3                      |
| Serializable Implementation | Locking Mechanism | Dependencies Detection |
| Default Isolation Level     | Repeatable Read   | Read Committed          |

What to keep in mind:
- **Retry mechanism: There might be errors, timeout or deadlock
- **Read Documentation: Each database engine might implement isolation level differently**
- **Reference for MySQL**: https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html
- **Reference for Postgres**: https://www.postgresql.org/docs/current/transaction-iso.html

#### 10. Setup Github Actions for Golang + Postgres to run automated tests
29/03/2023 (1h)

![[Pasted image 20230728163904.png]]
- Multiple types of trigger (event-based, scheduled/cron, manually/ui)
- A workflow is composed of multiple jobs which can run in parallel or sequentially if there are dependencies between jobs
- A job is composed of steps which are run sequentially and steps are composed of actions
	- Actions are sharable, meaning you can use actions defined by Github or other developers if needed
- Each job will be run on runners, which are basically servers ready to run the various scripts/actions (testing, building, etc...)
- In the end, runners will report status of jobs (failure, success, in progress, etc...) to Github, and this can be seen on the UI

Github Workflow
![[Pasted image 20230728163516.png]]

Github Runner
![[Pasted image 20230728163600.png]]

Github Job
![[Pasted image 20230728163702.png]]

Github Step
![[Pasted image 20230728163755.png]]

- Configuring Git
	- Adding the current repo to a repo of ngyale-pro
		- Github does not allow password identification for API call anymore
		- 1. Create a personal token: https://github.com/settings/tokens
		- 2. In your current local repo, `git init`
		- 3. Set the remote origin with `git add remote origin [url]`
		- 4. Pull the repo with `git pull --set-upstream origin main`
		- 5. Add every file to your commit `git add .` (You can add a .gitignore if not done already)
		- 6. Commit the files with `git commit -m "init: add source files"`
		- 7. Push your changes with `git push origin main`
	- "Fatal: Refusing to merge unrelated histories": `git pull origin master --allow-unrelated-histories`
		- https://www.educative.io/answers/the-fatal-refusing-to-merge-unrelated-histories-git-error
![[Pasted image 20230728170943.png|200]]
For more details:
- Caching your GitHub credentials in Git: https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git

```yaml
# This workflow will build a golang project
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-go

name: ci-test

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
  
    name: Test
    runs-on: ubuntu-latest

    # Service containers to run with `container-job`
    services:
      # Label used to access the service container
      postgres:
        # Docker Hub image
        image: postgres:15
        # Provide necessary env variables
        env:
          POSTGRES_USER: root
          POSTGRES_PASSWORD: secret
          POSTGRES_DB: simple_bank
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        # Maps tcp port 5432 on service container to the host
        ports:
          - 5432:5432

    steps:
    - name: Check out code into the Go module directory
      uses: actions/checkout@v3

    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.20'

    - name: Install golang-migrate
      run: |
        curl -L https://github.com/golang-migrate/migrate/releases/download/v4.16.2/migrate.linux-amd64.tar.gz | tar xvz
        sudo mv migrate /usr/bin
        which migrate
        
    - name: Run migrations
      run: make migrateup

    - name: Test
      run: make test
```

#### 12. Implement RESTful HTTP API in Go using Gin
31/07/2023 (1h)

| Web Framework    |  HTTP Router   |
| --- | --- |
| Gin   (Most Used)        | FastHTTP    |     
| Beego         | Gorilla Mux |     
| Echo          | HttpRouter  |     
| Revel         | Chi         |     
| Martini       |             |     
| Fiber         |             |     
| Buffalo       |             |     

Gin: A performant web framework in go
go get -u github.com/gin-gonic/gin

```go
// Server and route definition

package api

import (
	"github.com/gin-gonic/gin"
	db "github.com/ngyale-pro/simplebank/sqlc"
)

// Server serves HTTP requests for our baking service
type Server struct {
	store  *db.Store
	router *gin.Engine
}

func NewServer(store *db.Store) *Server {
	server := &Server{store: store}
	router := gin.Default()

	// Can give multiple functions as parameters, only the last one will be considered as the handler, the previous will act as middleware functions
	router.POST("/accounts", server.createAccount)
	router.GET("/accounts/:id", server.getAccount)
	router.GET("/accounts", server.listAccount)

	server.router = router
	return server
}

// Start runs the HTTP server on a specific address
func (server *Server) Start(address string) error {
	return server.router.Run(address)
}

func errorResponse(err error) gin.H {
	return gin.H{"error": err.Error()}
}

```

```go
// getAccount implementation

func (server *Server) getAccount(ctx *gin.Context) {
// Retrieve and validates the request body parameters
var req getAccountRequest
	if err := ctx.ShouldBindUri(&req); err != nil {
		ctx.JSON(http.StatusBadRequest, errorResponse(err))
		return
	}

// Call the internal function to retrieve the account
	account, err := server.store.GetAccount(ctx, req.ID)
	if err != nil {
		if err == sql.ErrNoRows {
			ctx.JSON(http.StatusNotFound, errorResponse(err))
			return
		}
		ctx.JSON(http.StatusInternalServerError, errorResponse(err))
		return
	}

// Return the account with a 200
	ctx.JSON(http.StatusOK, account)
}
```

- Test a POST API route with curl
curl -is -X POST http://localhost:8080/accounts -d '{"owner": "Alex", "currency": "EUR"}' -H "Content-Type: application/json"

- Multiple way to send parameters
	- URI -> (accounts/5)
	- Request body -> {"id": 5}
	- Query String -> ?id=5

Gin Input Parameter Validation 
- To bind a request body into a type, use model binding. We currently support binding of JSON, XML, YAML and standard form values (foo=bar&boo=baz).
- Note that you need to set the corresponding binding tag on all fields you want to bind. For example, when binding from JSON, set `json:"fieldname"`.
 Gin provides two sets of methods for binding:
- **Type** - Must bind -> Return an error with status code to 400
- **Type** - Should bind (The one we used)
    - **Methods** - `ShouldBind`, `ShouldBindJSON`, `ShouldBindXML`, `ShouldBindQuery`, `ShouldBindYAML`
    - **Behavior** - These methods use `ShouldBindWith` under the hood. If there is a binding error, the error is returned and it is the developer’s responsibility to handle the request and error appropriately.

Gin Documentation: https://gin-gonic.com/docs/examples/binding-and-validation/

```go
// Bind JSON (body) with conditions
// e.g {"owner: "alex", currency:"EUR"}
type createAccountRequest struct {
	Owner    string `json:"owner" binding:"required"`
	Currency string `json:"currency" binding:"required,oneof=USD EUR"` // Specifies that the value of the JSON must be either USD or EUR
}

var req createAccountRequest
if err := ctx.ShouldBindJSON(&req); err != nil 
{
		ctx.JSON(http.StatusBadRequest, errorResponse(err))
		return
}

// Bind Query String
// e.g: accounts?page_id=2?page_size=5
type listAccountRequest struct {
	PageID   int32 `form:"page_id" binding:"required,min=1"`
	PageSize int32 `form:"page_size" binding:"required,min=5,max=10"`
}

var req listAccountRequest
if err := ctx.ShouldBindQuery(&req); err != nil {
		ctx.JSON(http.StatusBadRequest, errorResponse(err))
	return
}

// Bind URI identifier
// e.g: /accounts/1
type getAccountRequest struct {
	ID int64 `uri:"id" binding:"required,min=1"`
}

var req getAccountRequest
if err := ctx.ShouldBindUri(&req); err != nil {
		ctx.JSON(http.StatusBadRequest, errorResponse(err))
		return
}

```

- Pagination
- SQLC configuration file to emit empty slices/list
```yaml
version: "2"
sql:
- schema: "./db/migration/"
  queries: "./db/query/"
  engine: "postgresql"
  gen:
    go: 
      package: "db"
      out: "sqlc"
      emit_json_tags: true
      emit_prepared_queries: false
      emit_interface: false
      emit_exact_table_names: false
      emit_empty_slices: true
  database:
    uri: "postgresql://postgres:postgres@localhost:5432/postgres"
  rules:
    - sqlc/db-prepare
```

#### 12. Load config from file & environment variables in Golang
31/07/2023 (30min)

Manage configuration with
- File: Easily specify default configuration for local development and testing
- Env Vars: Easily override the default configurations when deploying with docker containers

Viper: 
- Find, load unmarshal config file  -> JSON, TOML, YAML, ENV, INI
- Read config from env variables or flags -> Override existing values, set default values
- Read config from remote system -> etcd, consul
- Live watching and writing config file -> reread changed file, save any modifications

```bash
# app.env
DB_DRIVER="postgres"
DB_SOURCE="postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable"
SERVER_ADDRESS=0.0.0.0:8080
```


```go
package util

import "github.com/spf13/viper"

// Config stores all configuration of the application.
// The values are read by Viper from a config file or environment variables.
type Config struct {
	DBDriver      string `mapstructure:"DB_DRIVER"`
	DBSource      string `mapstructure:"DB_SOURCE"`
	ServerAddress string `mapstructure:"SERVER_ADDRESS"`
}

// Load Config reads configuration from file or environment variables
func LoadConfig(path string) (config Config, err error) {
	viper.AddConfigPath(path)
	viper.SetConfigName("app")
	viper.SetConfigType("env")

	// Autimatically override values it read from file
	viper.AutomaticEnv()

	err = viper.ReadInConfig()
	if err != nil {
		return
	}

	err = viper.Unmarshal(&config)
	return
}
```

- Viper can override the value in the configuration file with the value in the env variable
 - `SERVER_ADDRESS=0.0.0.0:8081 make server`

Viper Documentation: https://github.com/spf13/viper

#### 13. Mock DB for testing HTTP API in Go and achieve 100% coverage
01/08/2023 (1h)

gomock: a mocking framework for the [Go programming language](http://golang.org/). It integrates well with Go's built-in `testing` package, but can be used in other contexts too.
- https://github.com/golang/mock (No longer maintained, forked by uber)
- alternative: https://vektra.github.io/mockery/latest/

- Add a new directory in PATH in your ~/.zshrc
	- $PATH = $PATH:~/go/bin
- Take in account changes
	- source ~/.zshrc

mockgen -package mockdb -destination ./db/mock/store.go github.com/ngyale-pro/simplebank/db/sqlc Store 

Why mock ? 
- When not possible to have a production database ready
- Also faster than e2e test
- But add maintenance to do (need to maintain unit test AND mock test), Can also lead to overconfidence on system, though mock test != production grade test

Map Test Cases Definition to cover all usecase
```go
	testCases := []struct {
		name          string
		accountID     int64
		buildStubs    func(store *mockdb.MockStore)
		checkResponse func(t *testing.T, recorder *httptest.ResponseRecorder)
	}{
		{
			name:      "OK",
			accountID: account.ID,
			buildStubs: func(store *mockdb.MockStore) {
				store.EXPECT().GetAccount(gomock.Any(), gomock.Eq(account.ID)).Times(1).Return(account, nil)
			},
			checkResponse: func(t *testing.T, recorder *httptest.ResponseRecorder) {
				require.Equal(t, http.StatusOK, recorder.Code)
				requireBodyMatchAccount(t, recorder.Body, account)
			},
		},
		{
			name:      "NotFound",
			accountID: account.ID,
			buildStubs: func(store *mockdb.MockStore) {
				store.EXPECT().GetAccount(gomock.Any(), gomock.Eq(account.ID)).Times(1).Return(db.Account{}, sql.ErrNoRows)
			},
			checkResponse: func(t *testing.T, recorder *httptest.ResponseRecorder) {
				require.Equal(t, http.StatusNotFound, recorder.Code)
			},
		},
		{
			name:      "InternalError",
			accountID: account.ID,
			buildStubs: func(store *mockdb.MockStore) {
				store.EXPECT().GetAccount(gomock.Any(), gomock.Eq(account.ID)).Times(1).Return(db.Account{}, sql.ErrConnDone)
			},
			checkResponse: func(t *testing.T, recorder *httptest.ResponseRecorder) {
				require.Equal(t, http.StatusInternalServerError, recorder.Code)
			},
		},
		{
			name:      "BadRequest",
			accountID: 0,
			buildStubs: func(store *mockdb.MockStore) {
				store.EXPECT().GetAccount(gomock.Any(), gomock.Any()).Times(0)
			},
			checkResponse: func(t *testing.T, recorder *httptest.ResponseRecorder) {
				require.Equal(t, http.StatusBadRequest, recorder.Code)
			},
		},
	}
````

Test Execution -> Really clean
```go
	for _, testCase := range testCases {
		t.Run(testCase.name, func(t *testing.T) {
			ctrl := gomock.NewController(t)
			defer ctrl.Finish()

			store := mockdb.NewMockStore(ctrl)

			// Build stubs
			testCase.buildStubs(store)

			// Start test server and send request

			server := NewServer(store)
			recorder := httptest.NewRecorder()

			url := fmt.Sprintf("/accounts/%d", testCase.accountID)
			request, err := http.NewRequest(http.MethodGet, url, nil)
			require.NoError(t, err)

			server.router.ServeHTTP(recorder, request)
			// Check response
			testCase.checkResponse(t, recorder)
		})
	}
```

#### 14. Implement transfer money API with a custom params validator
01/08/2023 (30min)

- Implement a custom validator to avoid having to duplicate code

validator.go
```go
var validCurrency validator.Func = func(fieldLevel validator.FieldLevel) bool {
	if currency, ok := fieldLevel.Field().Interface().(string); ok {
		// check currency is supported
		return util.IsSupportedCurrency(currency)
	}
	return false
}
```

currency.go
```go
const (
	USD = "USD"
	EUR = "EUR"
	CAD = "CAD"
	YEN = "YEN"
)

// IsSupportedCurrency return true if the currency is supported
func IsSupportedCurrency(currency string) bool {
	switch currency {
	case USD, EUR, CAD, YEN:
		return true
	default:
		return false
	}
}
```

server.go
```go
	if v, ok := binding.Validator.Engine().(*validator.Validate); ok {
		v.RegisterValidation("currency", validCurrency)
	}
```

#### 15. Add users table with unique & foreign key constraints in PostgreSQL
![[Pasted image 20230801171315.png]]
```sql
CREATE TABLE "users" (
  "username" varchar PRIMARY KEY,
  "hashed_password" varchar NOT NULL,
  "full_name" varchar NOT NULL,
  "email" varchar UNIQUE NOT NULL,
  "password_changed_at" timestamptz NOT NULL DEFAULT '0001-01-01 00:00:00Z',
  "created_at" timestamptz NOT NULL DEFAULT (now())
);

-- One to many relation, a user can have multiple accounts
ALTER TABLE "accounts" ADD FOREIGN KEY ("owner") REFERENCES "users" ("username");

-- CREATE UNIQUE INDEX ON "accounts" ("owner", "currency"); Equivalent to the below, prevent from having two accounts with the same currency
ALTER TABLE "accounts" ADD CONSTRAINT "owner_currency_key" UNIQUE ("owner", "currency");

```

- To make it consistent in production, it is advised to create another migration just to add users rather than changing the whole schema, it allows to test upgrades and easily go back to previous states if this does not work.

#### 16. Handle DB errors in GO
- User Creation through the API

Status Code: What should I return for error such as:
- `"error": "pq: duplicate key value violates unique constraint \"owner_currency_key\""`
- `"error": "pq: insert or update on table \"accounts\" violates foreign key constraint \"accounts_owner_fkey\""`

These are errors violating constraint of the database, the possible choices are
- **400 (Bad Request)**: Server cannot or will not process the request due to something that is perceived to be a client error (for example, malformed request syntax, invalid request message framing, or deceptive request routing).
- **403 (Forbidden)**: Server understands the request but refuses to authorize it.
- **422 (Unprocessable Content)**: Server understands the content type of the request entity, and the syntax of the request entity is correct, but it was unable to process the contained instructions.

Methodology:
- 400 can't be used here, because the server understand the request (user creation, just it does not satisfy database constraints)
- 403 could be used, but forbidden seems like he does not have the rights to do it (while he does)
- 422 seemed the most appropriate here as the server understands the request, the syntax is correct but due to application logic, it can't be processed,

- In any case, in the industry, there does not seem to have a consensus on Status Code (and made lead to endless debate), so at the very least, the error should be return clearly, so the client know what to do.

#### 17. How to securely store password ? Hash password with BCrypt

Bcrypt: Password hashing function designed by Niels Provos and David Mazieres, presented at USENIX in 1999.
- Besides incorporating a salt to protect against rainbow table attacks, bcrypt is an adaptive function: over time, the iteration count can be increased to make it slower, so it remains resistant to brute force search attacks even with increasing computation power.
- Default password hash algorithm for OpenBSD

Function Definition: 
```
Function bcrypt
   Input:
      cost:     Number (4..31)                      log2(Iterations). e.g. 12 ==> 212 = 4,096 iterations
      salt:     array of Bytes (16 bytes)           random salt
      password: array of Bytes (1..72 bytes)        UTF-8 encoded password
   Output: 
      hash:     array of Bytes (24 bytes)
```


**Example:**
- password: `abc123xyz`
- cost: `12`
- salt: random()

![[Pasted image 20230802160054.png]]
Where:
- `$2a$`: The hash algorithm identifier (bcrypt)
- `12`: Input cost (212 i.e. 4096 rounds)
- `R9h/cIPz0gi.URNNX3kh2O`: A base-64 encoding of the input salt
- `PST9/PgBkqquzi.Ss7KIUgO2t0jWMUW`: A base-64 encoding of the first 23 bytes of the computed 24 byte hash

**Limitation**
- bcrypt has a maximum password length of 72 bytes.

![[Pasted image 20230802160423.png]]

#### 19. Why Paseto is better than JWT
![[Pasted image 20230803114235.png]]

- What is a JSON Web Token - JWT ?
![[Pasted image 20230803114252.png]]
	- Signing Algorithm
		- **Symmetric digital signature algorithm**
			- The same secret key is used to sign & verify token
			- For local use: internal services, where the secret key can be shared
			- HS256, HS384, HS512
				- HS256 = HMAC + SHA256
				- HMAC: Hash-Based Message Authentication Code
				- SHA: Secure Hash Algorithm
				- 256/384/512: Number of output bits
		- **Asymmetric digital signature algorithm**
			- The private key is used to sign token
			- The public key is used to verify token
			- For public use: internal service signs token, but external service needs to verify it
			- RS256 = RSA PKCSv1.5 + SHA256 `[PKCS: Public-Key Cryptography Standard]`
			- PS256 = RSA PSS + SHA256 `[PSS: Probabilistic Signature Scheme]`
			- ES256 = ECDSA + SHA256 `[ECDSA: Elliptic Curve Digital Signature Algorithm]`

What's the problem of JWT ?
- Weak algorithms
	- Give developer too many algorithms to choose
		- With some are known to be vulnerable
			- RSA PKCSv1.5: padding oracle attack
			- ECDSA: invalid-curse attack
	- Trivial Forgery
		- Set "alg" header to "none"
		- Set "alg" header to "HS256"(symmetric) while the server normally verifies token with a RSA public key (The hacker signs with the server's public key)
		- Crucial to check the header and see if it matches with the server
![[Pasted image 20230803115232.png]]


Paseto - Platform Agnostic Security Tokens
- Stronger Algorithms
	- Developers don't have to choose the algorithm
	- Only need to select the version of PASETO
	- Each version has 1 strong cipher suite
	- Only 2 most recent PASETO versions are accepted
- Non Trivial Forgery, no alg header
	- Everything is authenticated
	- Encrypted payload for local use

![[Pasted image 20230803115746.png]]

Structure of PASETO
![[Pasted image 20230803120117.png]]

#### 20. How to create and verify JWT & PASETO in Golang

Install uuid: `go get github.com/google/uuid`
Install jwt: `go get -u github.com/golang-jwt/jwt/v5`
Install paseto: `go get -u github.com/o1egl/paseto`

JWT Implementation:
- This library may be deprecated and can be updated to 
	- `github.com/golang-jwt/jwt/v5`
	- Keep in mind this library does not have the same interface as the old one, for instance, a Claim method should be implemented in your payload
```go
import "github.com/golang-jwt/jwt"

func NewJWTMaker(secretKey string) (Maker, error) {
	if len(secretKey) < minSecretKeySize {
		return nil, fmt.Errorf("invalid key size: must be at least %d characters", minSecretKeySize)
	}
	return JWTMaker{secretKey}, nil
}

func (maker JWTMaker) CreateToken(username string, duration time.Duration) (string, error) {
	payload, err := NewPayload(username, duration)
	if err != nil {
		return "", err
	}

	jwtToken := jwt.NewWithClaims(jwt.SigningMethodHS256, payload)
	return jwtToken.SignedString([]byte(maker.secretKey))
}

func (maker JWTMaker) VerifyToken(token string) (*Payload, error) {

	keyFunc := func(token *jwt.Token) (interface{}, error) {
		_, ok := token.Method.(*jwt.SigningMethodHMAC)
		if !ok {
			return nil, ErrInvalidToken
		}
		return []byte(maker.secretKey), nil
	}
	jwtToken, err := jwt.ParseWithClaims(token, &Payload{}, keyFunc)
	if err != nil {
		verr, ok := err.(*jwt.ValidationError)
		// There are two possible scenarios here in case of error: Expired Token or Invalid Token
		// Checks in the error stack if there is an ErrExpiredToken at some point (the one coded in Valid()), if there is, then we return an error ErrExpiredToken, otherwise we return ErrInvalidToken
		if ok && errors.Is(verr.Inner, ErrExpiredToken) {
			return nil, ErrExpiredToken
		}
		return nil, ErrInvalidToken
	}

	payload, ok := jwtToken.Claims.(*Payload)
	if !ok {
		return nil, ErrInvalidToken
	}

	return paylo
```

Token Payload (id, username, issued_at, expired_at) // It's possible to add more field to it depending on your usecase
```go

// Different types of error
var (
	ErrExpiredToken = errors.New("token has expired")
	ErrInvalidToken = errors.New("token is invalid")
)

type Payload struct {
	ID        uuid.UUID `json:"id"`
	Username  string    `json:"username"`
	IssuedAt  time.Time `json:"issued_at"`
	ExpiredAt time.Time `json:"expired_at"`
}

// NewPayload creates a new token payload with a specific username and duration
func NewPayload(username string, duration time.Duration) (*Payload, error) {
	tokenID, err := uuid.NewRandom()
	if err != nil {
		return nil, err
	}

	payload := &Payload{
		ID:        tokenID,
		Username:  username,
		IssuedAt:  time.Now(),
		ExpiredAt: time.Now().Add(duration),
	}
	return payload, nil
}

func (payload *Payload) Valid() error {
	if time.Now().After(payload.ExpiredAt) {
		return ErrExpiredToken
	}
	return nil
}
```

Paseto implementation, requires less verification as the version of paseto specifies which algorithm to encrypt, there is no need for the user to check it all the time (compared to JWT where there is a need to check this alg header)
```go
type PasetoMaker struct {
	paseto       *paseto.V2
	symmetricKey []byte
}

func NewPasetoMaker(symmetricKey string) (Maker, error) {
	if len(symmetricKey) != chacha20poly1305.KeySize {
		return nil, fmt.Errorf("invalid key size: must be exactly %d characters", chacha20poly1305.KeySize)
	}

	maker := &PasetoMaker{
		paseto:       paseto.NewV2(),
		symmetricKey: []byte(symmetricKey),
	}
	return maker, nil
}

// CreateToken creates a new token for a specific username and duration
func (maker PasetoMaker) CreateToken(username string, duration time.Duration) (string, error) {
	payload, err := NewPayload(username, duration)
	if err != nil {
		return "", err
	}
	return maker.paseto.Encrypt(maker.symmetricKey, payload, nil)
}

// VerifyToken checks if the token is valid or not
func (maker PasetoMaker) VerifyToken(token string) (*Payload, error) {
	payload := &Payload{}

	err := maker.paseto.Decrypt(token, maker.symmetricKey, payload, nil)
	if err != nil {
		return nil, ErrInvalidToken
	}

	err = payload.Valid()
	if err != nil {
		return nil, err
	}

	return payload, nil
}

```

Some JWT tests
```go
import (
	"testing"
	"time"

	"github.com/golang-jwt/jwt"
	"github.com/ngyale-pro/simplebank/util"
	"github.com/stretchr/testify/require"
)

func TestJWTMaker(t *testing.T) {
	maker, err := NewJWTMaker(util.RandomString(32))
	require.NoError(t, err)

	username := util.RandomName()
	duration := time.Minute

	issuedAt := time.Now()
	expiredAt := time.Now().Add(duration)

	token, err := maker.CreateToken(username, duration)
	require.NoError(t, err)
	require.NotEmpty(t, token)

	payload, err := maker.VerifyToken(token)
	require.NoError(t, err)
	require.NotEmpty(t, payload)

	require.NotZero(t, payload.ID)
	require.Equal(t, username, payload.Username)
	require.WithinDuration(t, issuedAt, payload.IssuedAt, time.Second)
	require.WithinDuration(t, expiredAt, payload.ExpiredAt, time.Second)
}

func TestExpiredJWTToken(t *testing.T) {
	maker, err := NewJWTMaker(util.RandomString(32))
	require.NoError(t, err)

	username := util.RandomName()
	duration := -time.Minute

	token, err := maker.CreateToken(username, duration)
	require.NoError(t, err)
	require.NotEmpty(t, token)

	payload, err := maker.VerifyToken(token)
	require.Error(t, err)
	require.EqualError(t, err, ErrExpiredToken.Error())
	require.Nil(t, payload)
}

func TestInvalidJWTTokenAlgNone(t *testing.T) {
	payload, err := NewPayload(util.RandomName(), time.Minute)
	require.NoError(t, err)

	jwtToken := jwt.NewWithClaims(jwt.SigningMethodNone, payload)
	token, err := jwtToken.SignedString(jwt.UnsafeAllowNoneSignatureType)

	require.NoError(t, err)
	require.NotEmpty(t, token)

	maker, err := NewJWTMaker(util.RandomString(32))
	require.NoError(t, err)

	payload, err = maker.VerifyToken(token)
	require.EqualError(t, err, ErrInvalidToken.Error())
	require.Nil(t, payload)
}

```

#### 22. How to implement authentication middleware and authorization rules in Golang

- What is a middleware ?
Something that sits between the original request and the final handler, usually
- Logger()
- Authentication()
- Authorization()
- etc...
![[Pasted image 20230804144630.png]]


![[Pasted image 20230804154334.png]]

#### 23. Build a minimal Golang Docker image with a multistage Dockerfile

git checkout -b ft/docker

Golang Docker Image: https://hub.docker.com/_/golang

```dockerfile
FROM golang:1.20-alpine3.18 # From [image]
WORKDIR /app 
COPY . . # Current Repo to Workdir
RUN go build -o main main.go
  
# Not really opening this port, rather documenting it

EXPOSE 8080
CMD ["/app/main"]
```

- Reduce the size with a multi stage docker file
	- Build the image
	- Only keep the image

```dockerfile
# Build stage
FROM golang:1.20-alpine3.18 AS builder
WORKDIR /app
COPY . .
RUN go build -o main main.go

# Run stage
FROM alpine:3.18
WORKDIR /app
COPY --from=builder /app/main .

# Not really opening this port, rather documenting it
EXPOSE 8080
CMD ["/app/main"]
```

```bash
# List all image
docker images

# Delete the image using its id
docker rmi de0a68595e97 

# Build a docker image with a tag
docker build -t simplebank:latest .

# Run a named docker container from an image tag and publish/expose a specified port
docker run --name simplebank -p 8080:8080 simplebank:latest

# Set an environment variable
docker run --name simplebank -p 8080:8080 -e GIN_MODE=release simplebank:latest

# List running docker containers
docker ps
docker ps -a # List all container, even stopped

# Delete a container with its id
docker rm [container_id]


```

#### 24. How to use docker network to connect 2 stand-alone containers

https://www.youtube.com/watch?v=VcFnqQarpjI&list=PLy_6D98if3ULEtXtNSY_2qN21VCKgoQAE&index=24

Two docker containers:
- simplebank
- postgresql

How can I make simplebank communicate with postgresql ? 
- By default the DB_SOURCE: "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable"
	- The localhost address won't work anymore

By using `docker container inspect postgres15`
- I can access the configuration of the container and find its IP address in the NetworkSettings
- I can see the address of the container is 172.17.0.2

Because simplebank is using Viper, i can easily override the value of DB_SOURCE in the app.env by setting an environnement variable

Running the following command will do the trick

```bash
docker run --name simplebank -p 8080:8080 -e GIN_MODE=release -e DB_SOURCE='postgresql://root:secret@172.17.0.2:5432/simple_bank?sslmode=disable' simplebank:latest
```

**However, doing so is not effective, as the IP address of the container may change in the future, so there is a need to lookup every time the IP address which can be bothersome**

**The right way to do so is to have your container in the same docker network and using their name, they will be able to resolve to their IP address. However, this does not work on the default network "bridge". So it's necessary to create a new one.**

It's okay for container to be connected to multiple network

```bash
# List all network in docker
docker network ls

# Inspect a specific network
docker network inspect bridge 

# Create a new docker network
docker network create bank-network

# Connect a docker container to a new network
docker network connect bank-network postgres15

```

// bridge network
```json
        "Containers": {
            "9244abb3c065d944d7a5e339973e0959b70319e65d6c1f70519212984e239ce0": {
                "Name": "postgres15",
                "EndpointID": "42d529e5c9788c0ef6b0d3ef1a0b23e54a21a98a5464239585fb7c5bff2cd6d9",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
```


// NetworkSettings after docker container inspect, alternatively lazydocker will display the config also
```json
"NetworkSettings": {
...
            "Networks": {
                "bridge": {
                    "IPAMConfig": null,
                    "Links": null,
                    "Aliases": null,
                    "NetworkID": "8786a9d8f04bc50b67d06320c801eae54d3e5b08cc76b1400b2343e3cfb2e7fa",
                    "EndpointID": "42d529e5c9788c0ef6b0d3ef1a0b23e54a21a98a5464239585fb7c5bff2cd6d9",
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.2",
                    "IPPrefixLen": 16,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "MacAddress": "02:42:ac:11:00:02",
                    "DriverOpts": null
                }
            }
        }
```

#### 25. How to write docker-compose file and control service start up orders with waitfor.sh
Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.

Compose works in all environments: production, staging, development, testing, as well as CI workflows. It also has commands for managing the whole lifecycle of your application:

- Start, stop, and rebuild services
- View the status of running services
- Stream the log output of running services
- Run a one-off command on a service

> In the previous example, rather than having to create the network, then create the two containers separately and connect it to the same network with multiple commands, using docker-compose will allow to unify the whole process in order to have one file which will describe how to setup the whole SimpleBank


Add executable write to anyone (admin, group, user) for start.sh: `chmod +x start.sh`

```bash
# Build images and run network, containers with compose
docker compose up

# Stop services, containers
docker compose down

# To do another docker compose up, it's also necessary to delete the image created
docker rmi [composed_image]
```

Minimal docker compose file to have simple bank running in the same network exposing the api on 8080
```yaml
version: "3.8"
services:
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=secret
      - POSTGRES_DB=simple_bank
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U root -d simple_bank" ]
      interval: 5s
      timeout: 5s
      retries: 5

  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports: [ "8080:8080" ]
    environment:
      - DB_SOURCE=postgresql://root:secret@postgres:5432/simple_bank?sslmode=disable
    depends_on:
      postgres:
        condition: service_healthy
```

> There are several things to be aware of when using `depends_on`:
> - `depends_on` does not wait for `db` and `redis` to be “ready” before starting `web` - only until they have been started. If you need to wait for a service to be ready, see [Controlling startup order](https://docs.docker.com/compose/startup-order/) for more on this problem and strategies for solving it.
> - The `depends_on` option is ignored when [deploying a stack in swarm mode](https://docs.docker.com/engine/reference/commandline/stack_deploy/) with a version 3 Compose file.

**In our case, we defined a healthcheck in the postgres service. So in the case it's healthy, the condition used (service_healthy) for the depends_on: postgres is fulfilled, the api service can start to run, otherwise it will wait for new retries

There is another alternative which is not Docker dependant, using the wait-for script: https://github.com/eficode/wait-for
The following code implements this solution. Keep in mind, using entrypoint in docker compose overrides the one in the Dockerfile, this is why there is a command to execute the same command that was in the Dockerfile

```yaml
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports: [ "8080:8080" ]
    environment:
      - DB_SOURCE=postgresql://root:secret@postgres:5432/simple_bank?sslmode=disable
    depends_on:
    entrypoint: [ "/app/wait-for.sh", "postgres:5432", "--", "/app/start.sh"]
    command: [ "app/main"]
```

#### 27. Auto Build & Push Docker Image to AWS ECR with Github Actions

```bash
# Retrieve an authentication token and authenticate your Docker client to your registry
aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 814575821063.dkr.ecr.eu-west-1.amazonaws.com

# Build your docker image
docker build -t simple-bank .

# Tag your image to push it to the repository
docker tag simple-bank:latest 814575821063.dkr.ecr.eu-west-1.amazonaws.com/simple-bank:latest

# Push the image
docker push 814575821063.dkr.ecr.eu-west-1.amazonaws.com/simple-bank:latest
```

Don't push the image directly from your development machine
-> Build it and push it using a CI (Github Actions)

Github Actions Marketplace: https://github.com/marketplace?type=actions

Terraform OIDC Github: https://registry.terraform.io/modules/unfunco/oidc-github/aws/latest

Github Branch Protection:
- For main:
	- Require status checks (tests) to pass before merging
	- Require branches to be up to date before merging
	- This allows to ensure unit tests shall pass before altering main.

Github Actions to build and image and push it to Amazon ECR Repository:
```yaml
# This workflow will build an image and push it to an Amazon ECR repository
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-go

name: Deploy to production

on:
  push:
    branches: [ "main" ]

jobs:
  build:
    name:  Build image
    runs-on: ubuntu-latest
    # These permissions are needed to interact with GitHub's OIDC Token endpoint.
    permissions:
      id-token: write
      contents: read

    steps:
		# This retrieve the last commit (files) and authenticates for any future git commands, it's usually mandatory
      - name: Check out code 
        uses: actions/checkout@v3
		# There are two ways to configure AWS credentials, one is using access key of a user, the more secured way is using the OIDC token between AWS and Github. 
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::814575821063:role/GithubActionsRole
          aws-region: eu-west-1
      # - name: Configure AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v2
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_SDBX }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_SDBX }}
      #     aws-region: eu-west-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
        with:
          mask-password: 'true'
      - name: Build, tag, and push docker image to Amazon ECR
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          REPOSITORY: simple-bank
          IMAGE_TAG: ${{ github.sha }} # Associate the commit ID to the image as a tag
        run: |
          docker build -t $REGISTRY/$REPOSITORY:$IMAGE_TAG .
          docker push $REGISTRY/$REPOSITORY:$IMAGE_TAG
```